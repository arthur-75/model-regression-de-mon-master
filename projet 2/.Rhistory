dnorm(vect1)
vect1
plot(gra1)
gra1 = dnorm(vect1)
plot(gra1,type="l")
lines(dt(vect1), col="red")
dt(vect1)
dt(vect1,5)
lines(dt(vect1,5), col="red")
plot(dnorm(vect1,5),type="l")
lines(dt(vect1,5), col="red")
lines(dt(vect1,30), col="green")
plot(dnorm(vect1),type="l")
lines(dt(vect1,5), col="red")
lines(dt(vect1,30), col="green")
legend("topleft",col = c("black","red","green"),legend = c("gaussienne","T à 5 DL","T à 30 DL"),lty =c(1,1) )
legend("topleft",col = c("black","red","green"),legend = c("gaussienne","T à 5 DL","T à 30 DL"),lty =c(1,1,1) )
plot(dnorm(vect1),type="l")
lines(dt(vect1,5), col="red")
lines(dt(vect1,30), col="green")
legend("topleft",col = c("black","red","green"),legend = c("gaussienne","T à 5 DL","T à 30 DL"),lty =c(1,1,1) )
vect1= seq(-3,3,0.01)
plot(dnorm(vect1),type="l")
lines(dt(vect1,5), col="red")
lines(dt(vect1,30), col="green")
legend("topleft",col = c("black","red","green"),legend = c("gaussienne","T à 5 DL","T à 30 DL"),lty =c(1,1,1) )
#3 X_t    = Z_t  + theta Z_{t-1}MA(1)
n = 100
theta = -1
modelMA1 = list(ma=c(theta))
X = arima.sim(n= n,model= modelMA1,sd=sdZ)
plot(X,main = paste("theta=" , theta))
plot(X,main = paste("theta=" , theta))
acf(X)
pacf(X)
#remarque 	gamma(1) = Cov(Xt-1,Xt) = ... = theta.sigma^2
# 			gamma(0) = var(Xt) = var(Zt + theta Zt-A) = sigma^2 + theta sigma^2
# 		 rho(1) = gamma(1)/gamm(0) = theta/(1+theta)
theta/(1+abs(theta))
#5 Identification de l'ordre d'un AR: X_t -1/3.X_{t-1} + 1/3 X_{t-2}-1/3 X_{t-3} .... +1/3X_{t-12} = Z_t
n = 1000
rep(c(1,-1),6)
ar= rep(c(1,-1),6) * rep(1/3,12)
ar
ar= rep(c(1,-1),6) /3
ar
abs(polyroot(c(1,-ar)))
polyroot(c(1,-ar))
ar
ar= c(13/3, -4/3)
abs(polyroot(c(1,-ar)))
ar= c(7/12, -1/12)
abs(polyroot(c(1,-ar)))
ar= c(-7/12, 5/4)
abs(polyroot(c(1,-ar)))
ar= c(3/10, -7/12)
abs(polyroot(c(1,-ar)))
ma
ma =c(-7/10,1/10)
abs(polyroot(c(1,ma)))
ma =c(-1/10,5/12)
abs(polyroot(c(1,ma)))
c(-7/10,1/10)
ma
ma= rep(c(-1,1),6) * rep(3/4,4)
ma
install.packages("swirl")
library(swirl)
swirl()
plot(child ~parent ,galton)
plot(jitter(child,4) ~ parent,galton)
regrline <- lm(child ~ parent, galton)
abline(regrline, lwd=3, col='red')
summary(regrline)
# Regress the given variable on the given predictor,
# suppressing the intercept, and return the residual.
regressOneOnOne <- function(predictor, other, dataframe){
# Point A. Create a formula such as Girth ~ Height -1
formula <- paste0(other, " ~ ", predictor, " - 1")
# Use the formula in a regression and return the residual.
resid(lm(formula, dataframe))
}
# Regress the given variable on the given predictor,
# suppressing the intercept, and return the residual.
regressOneOnOne <- function(predictor, other, dataframe){
# Point A. Create a formula such as Girth ~ Height -1
formula <- paste0(other, " ~ ", predictor, " - 1")
# Use the formula in a regression and return the residual.
resid(lm(formula, dataframe))
}
d
21
bye()
d
# Regress the given variable on the given predictor,
# suppressing the intercept, and return the residual.
regressOneOnOne <- function(predictor, other, dataframe){
# Point A. Create a formula such as Girth ~ Height -1
formula <- paste0(other, " ~ ", predictor, " - 1")
# Use the formula in a regression and return the residual.
resid(lm(formula, dataframe))
}
# Eliminate the specified predictor from the dataframe by
# regressing all other variables on that predictor
# and returning a data frame containing the residuals
# of those regressions.
eliminate <- function(predictor, dataframe){
# Find the names of all columns except the predictor.
others <- setdiff(names(dataframe), predictor)
# Calculate the residuals of each when regressed against the given predictor
temp <- sapply(others, function(other)regressOneOnOne(predictor, other, dataframe))
# sapply returns a matrix of residuals; convert to a data frame and return.
as.data.frame(temp)
}
# Eliminate the specified predictor from the dataframe by
# regressing all other variables on that predictor
# and returning a data frame containing the residuals
# of those regressions.
eliminate <- function(predictor, dataframe){
# Find the names of all columns except the predictor.
others <- setdiff(names(dataframe), predictor)
# Calculate the residuals of each when regressed against the given predictor
temp <- sapply(others, function(other)regressOneOnOne(predictor, other, dataframe))
# sapply returns a matrix of residuals; convert to a data frame and return.
as.data.frame(temp)
}
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla")
numbers <- c(1:10)
numbers
udacious
numbers <- c(numbers, 11:20)
numbers
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla", YOUR_NAME)
# 2. Replace YOUR_NAME with your actual name in the vector
# 'udacious' and run the code. Be sure to use quotes around it.
YOUR_NAME<-"Dan"
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla", YOUR_NAME)
udacious
mystery = nchar(udacious)
mystery
mystery == 11
udacious[mystery == 11]
data(mtcars)
names(mtcars)
?mtcars
mtcars
mean(mtcars$mpg)
getwd()
subset(mtcars,cyl==4)
subset(mtcars,cyl==4,vs=1)
subset(mtcars,cyl==4 and vs=1)
subset(mtcars,cyl==4, vs=1)
subset(mtcars,cyl==4, vs=1,am=0)
set.seed(132)
train.samples1=createDataPartition(credit1$class,p =.8, list =F)#train 80%
train1<- credit1[train.samples1, ] # 80%
#telecharger les package
#install.packages("corrplot")
#install.packages("RColorBrewer")
#install.packages("caret")
#install.packages("MASS")
#install.packages("survey")
#install.packages("knitr")
#install.packages("ROCR")
library(corrplot) # pour correlation
library(RColorBrewer)  #pour graph correlation
library(caret) # Pour test et train
library(MASS) # pour Stepwise
library(survey) # wald test
library(knitr)# pour tables
library(ROCR) # performance de cutOff
#Question 1
#--------------------------------------------------------------------
#on donne un nom pour chaque variable
noms_variables <- c("status_account","duration","history","purpose",
"amount","savings","employment","installement_rate",
"status_personnal","other_debtors","residence_since",
"property","age","other_installment","housing",
"nb_credits","job","liable","telephone","foreign","class")
credit0 <- read.table("GermanCredit.txt", col.names = noms_variables)
credit0$class <- as.factor(credit0$class-1) # rendre les valures de class à 0, 1
credit <- subset(credit0, select=-c(foreign))#supprimer la variable foreign
any(!is.na(credit))# pas de manque de données
#changer de char à factor pour les variables qualitatives
credit1<- lapply(credit, function(x) if(class(x) == "character") as.factor(x) else x)
credit1 <- data.frame(credit1) # rendre en data.frame
head(credit1,4) #pour avoir une idée
str(credit1$class)
#Question 2
#--------------------------------------------------------------------
#Cette functinn va decrire les données et va faire une repésenation graphiques
decr <- function(data){
str(data)# bref des varibales
print("Les fréquences marginals : ")
for (i in 1:dim(data)[2]){
if (class(data[,i]) == "factor"){ #pour les variables qualitatives
tab =table(data[,i])#fréquences
barplot(tab,col=1:length(tab),main=names(data)[i])
print(names(data)[i]) # nom de variable
print(prop.table(tab)) #fréquences marginals
print("")
}
else { #pour les variables quantitatives
boxplot(data[,i],main=names(data)[i])
}
}
print("")
summary(data) #bref statistique
}
decr(credit1) # function pour décrire le jeu de données
qunt=credit1[,c(2,5,8,11,13,16,18)]# que les variables quantitatives
cov(qunt)#covaraince et varaince
#correlation
corrplot(cor(qunt), type="upper", order="hclust",
col=brewer.pal(n=8, name="RdYlBu"))
setwd("~/OneDrive - Université paris nanterre/Fac/Nanterre/model regression/projet 2")
#telecharger les package
#install.packages("corrplot")
#install.packages("RColorBrewer")
#install.packages("caret")
#install.packages("MASS")
#install.packages("survey")
#install.packages("knitr")
#install.packages("ROCR")
library(corrplot) # pour correlation
library(RColorBrewer)  #pour graph correlation
library(caret) # Pour test et train
library(MASS) # pour Stepwise
library(survey) # wald test
library(knitr)# pour tables
library(ROCR) # performance de cutOff
#Question 1
#--------------------------------------------------------------------
#on donne un nom pour chaque variable
noms_variables <- c("status_account","duration","history","purpose",
"amount","savings","employment","installement_rate",
"status_personnal","other_debtors","residence_since",
"property","age","other_installment","housing",
"nb_credits","job","liable","telephone","foreign","class")
credit0 <- read.table("GermanCredit.txt", col.names = noms_variables)
credit0$class <- as.factor(credit0$class-1) # rendre les valures de class à 0, 1
credit <- subset(credit0, select=-c(foreign))#supprimer la variable foreign
any(!is.na(credit))# pas de manque de données
#changer de char à factor pour les variables qualitatives
credit1<- lapply(credit, function(x) if(class(x) == "character") as.factor(x) else x)
credit1 <- data.frame(credit1) # rendre en data.frame
head(credit1,4) #pour avoir une idée
str(credit1$class)
#Question 2
#--------------------------------------------------------------------
#Cette functinn va decrire les données et va faire une repésenation graphiques
decr <- function(data){
str(data)# bref des varibales
print("Les fréquences marginals : ")
for (i in 1:dim(data)[2]){
if (class(data[,i]) == "factor"){ #pour les variables qualitatives
tab =table(data[,i])#fréquences
barplot(tab,col=1:length(tab),main=names(data)[i])
print(names(data)[i]) # nom de variable
print(prop.table(tab)) #fréquences marginals
print("")
}
else { #pour les variables quantitatives
boxplot(data[,i],main=names(data)[i])
}
}
print("")
summary(data) #bref statistique
}
decr(credit1) # function pour décrire le jeu de données
qunt=credit1[,c(2,5,8,11,13,16,18)]# que les variables quantitatives
cov(qunt)#covaraince et varaince
#correlation
corrplot(cor(qunt), type="upper", order="hclust",
col=brewer.pal(n=8, name="RdYlBu"))
#question 3
#--------------------------------------------------------------------
modReg <- (glm(class ~ .,data = credit1,family = binomial(link='logit')))
#question 4
#--------------------------------------------------------------------
summary(modReg)
#Ensuit on veut savoir si les varaibles sont representives
for(g in 1:dim(credit1)[2]){
if (class(credit1[,g])=="factor"){
print(names(credit1[g]))
print(xtabs(~class+ credit1[,g],data=credit1)) #representive ?
print('')}
}
#Il faut noter que certain valuers dans les variables :
#purpose, history, other_debtors job , other_installment
# ne sont pas tres repesentives par rapport aux autres variable
#-------------------------------------------------------------------
# wald test
t=1
while(t < 20){
print(regTermTest(modReg,names(credit1)[t]));  t=t+1 ; print('')}
# McFadden's Pseudo R^2
ll.null <- modReg$null.deviance/-2
ll.propse<- modReg$deviance/-2
Pseudo_R2 <-  (ll.null-ll.propse )/ll.null
Pseudo_R2 # et donc tres faible juste 26% represent le modele
#P-value pour chi2
Chi2_R2 <- 1- pchisq(2*(ll.propse-ll.null),df=length(modReg$coefficients)-1)
Chi2_R2 # 0
anova(modReg, test="Chisq")
namess<- c("gen.AIC","gen.R2","gen.Acc","BIC_gen",
"setp.AIC","setp.R2","setp.Acc","BIC_setp",
"AIC_BIC","R2_BIC","Acc_BIC","BIC_BIC",
"anova.AIC","anova.R2","anova.Acc","BIC_anova")
#-----------------------------------------------------------------------------
Trying_all <- function(data1,methode,n.iter){
namess<- c("gen.AIC","gen.R2","gen.Acc","BIC_gen",
"setp.AIC","setp.R2","setp.Acc","BIC_setp",
"AIC_BIC","R2_BIC","Acc_BIC","BIC_BIC",
"anova.AIC","anova.R2","anova.Acc","BIC_anova")
data_sel<- data.frame(matrix(0,0,16))
names(data_sel)<- namess
#Loooop
for (i in 1:n.iter){
# tout d'abord Il est important de deviser les données en deux parties
# * Train 80% de données
# * Test 20% de données
#Pour bien evaluer le modele par rapport l'Accuracy;
set.seed(123+i)
train.samples=createDataPartition(data1$class,p =.8, list =F)#train 80%
train.data1<- data1[train.samples, ] # 80%
test.data1 <- data1[-train.samples, ] # 20%
#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#General Methode où il y a toutes les variables
model_gen <- glm(class ~., data = train.data1,
family = binomial(link=methode)) #modelisation
probabilities <- predict(model_gen,test.data1,
type = "response")#prediction le test 20%
predicted.class <- ifelse(probabilities > 0.5, 1, 0)# Good ou Bad credit
R2_gen=(model_gen$null.deviance-model_gen$deviance)/model_gen$null.deviance #Pseudo  R2
Acc_gen =mean(predicted.class==test.data1$class) #Accuracy
AIC_gen=model_gen$aic #AIC
BIC_gen= BIC(model_gen) #BIC
#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#Methode Stepwise minimiser AIC sur le modele : model_general
model_setp<- stepAIC(model_gen, direction = "both",
trace = FALSE) #modelisation
probabilities <- predict(model_setp,test.data1,
type = "response")#prediction le test 20%
predicted.class <- ifelse(probabilities > 0.5, 1, 0)# Good ou Bad credit
R2_setp=(model_setp$null.deviance-model_setp$deviance)/model_setp$null.deviance#Pseudo  R2
Acc_setp=mean(predicted.class==test.data1$class)#Accuracy
AIC_setp=model_setp$aic #AIC
BIC_setp= BIC(model_setp)#BIC
#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#Methode Stepwise minimiser BIC sur le modele : model_general
model.BIC <- stepAIC(model_gen, k = log(nrow(train.data1)),
trace = FALSE) #modelisation
probabilities <- predict(model.BIC,test.data1,
direction = "both", type = "response")#prediction le test 20%
predicted.class <- ifelse(probabilities > 0.5, 1, 0)# Good ou Bad credit
R2_BIC=(model.BIC$null.deviance-model.BIC$deviance)/model.BIC$null.deviance #Pseudo  R2
Acc_BIC= mean(predicted.class==test.data1$class)#Accuracy
AIC_BIC=model.BIC$aic #AIC
BIC_BIC= BIC(model.BIC)#BIC
#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#Methode Anova personnal
#Analyse apres utilser toute variables ont une Deviance plus elevé et que
#les variables signficants d'apres wald test
model_anova <-(glm(class ~status_account+duration+history+purpose+savings
+installement_rate+status_personnal+other_debtors+ other_installment,
data = train.data1, family = binomial(link=methode))) #modelisation
probabilities <- predict(model_anova,test.data1,
type = "response")#prediction le test 20%
predicted.class <- ifelse(probabilities > 0.5, 1, 0) # Good ou Bad credit
R2_anova=(model_anova$null.deviance-model_anova$deviance)/model_anova$null.deviance#Pseudo  R2
Acc_anova= mean(predicted.class==test.data1$class)#Accuracy
AIC_anova=model_anova$aic #AIC
BIC_anova= BIC(model_anova)#BIC
#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
#Mettre la data ensemeble
new.data <- data.frame(AIC_gen,R2_gen,Acc_gen,BIC_gen
,AIC_setp,R2_setp,Acc_setp,BIC_setp,
AIC_BIC,R2_BIC,Acc_BIC,BIC_BIC,
AIC_anova,R2_anova,Acc_anova,BIC_anova)
names(new.data)<-namess
data_sel<-rbind(data_sel,new.data )
}
return(data_sel)
}
# Monto Carlo de logit
data_sel.logit<-Trying_all(credit1,"logit",n.iter=100)#Attention ca durer 8 min
# mean  de logit
mean_data_sel.logit 	= data.frame(lapply(data_sel.logit,mean))
#97.5% de logit
Up_data_sel.logit	= data.frame(lapply(data_sel.logit,
function(x) quantile(x,0.975)))
#2.5% de logit
Low_data_sel.logit =data.frame(lapply(data_sel.logit,
function(x) quantile(x,0.025)))
# Monto Carlo de probit
data_sel.probit<-Trying_all(credit1,"probit",n.iter=100)#Attention ca durer 8 min
#mean de probit
mean_data_sel.probit 	= data.frame(lapply(data_sel.probit,mean))
#97.5% de probit
Up_data_sel.probit	= data.frame(lapply(data_sel.probit,
function(x) quantile(x,0.975)))
#2.5% de probit
Low_data_sel.probit=data.frame(lapply(data_sel.probit,
function(x) quantile(x,0.025)))
# Monto Carlo de cloglog
data_sel.cloglog<-Trying_all(credit1,"cloglog",n.iter=100)#Attention ca durer 8 min
a
# meanet de cloglog
mean_data_sel.cloglog 	= data.frame(lapply(data_sel.cloglog,mean))
# 97.5% de cloglog
Up_data_sel.cloglog 	= data.frame(lapply(data_sel.cloglog,
function(x) quantile(x,0.975)))
#2.5% de cloglog
Low_data_sel.cloglog =data.frame(lapply(data_sel.cloglog,
function(x) quantile(x,0.025)))
# mean de tous les link fonction ensemble
data.mean=rbind(mean_data_sel.logit,mean_data_sel.probit,mean_data_sel.cloglog)
row.names(data.mean)<-c("logit","probit","cloglog")
#premiere 8 valeurs (pour mieux voir)
knitr::kable(data.mean[1:8],digits=4,
caption = "Mean pour les Modeles : general x 4, stepwise min AIC x 4")
#deuixeme 8 valeurs (pour mieux voir)
knitr::kable(data.mean[9:16],digits=4,
caption ="Mean pour les Modeles :stepwise min BIC x 4 , perso Anova x 4 ")
#"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
#Quantile "97.5%"
data.97.5=rbind(Up_data_sel.logit,Up_data_sel.probit ,Up_data_sel.cloglog)
row.names(data.97.5)<-c("logit","probit","cloglog")
knitr::kable(data.97.5[1:8],digits=4,
caption = "Quantile 97.5% pour les Modeles : general x 4, stepwise min AIC x 4")
knitr::kable(data.97.5[9:16],digits=4,
caption ="Quantile 97.5% pour les Modeles :stepwise min BIC x 4 , perso Anova x 4")
#"""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""
#Quantile "2.5%"
data.2.5=rbind(Low_data_sel.logit,Low_data_sel.probit ,Low_data_sel.cloglog)
row.names(data.2.5)<-c("logit","probit","cloglog")
knitr::kable(data.2.5[1:8],digits=4,
caption = "Quantile 2.5% pour les Modeles : general x 4, stepwise min AIC x 4")
knitr::kable(data.2.5[9:16],digits=4,
caption ="Quantile 2.5% pour les Modeles :stepwise min BIC x 4 , perso Anova x 4")
#Plot ##################################@
#Accurancy
plot(time(data_sel.logit$gen.Acc),data_sel.logit$gen.Acc,type="l",
main="Comparison Accurancy modelisation Logite")
#modèle d'apres  Step-min_AIc logit
model_Step <- glm(formula = class ~ status_account + duration + history + purpose +
amount + savings + employment + installement_rate + status_personnal +
other_debtors + age + other_installment + housing, family = binomial(link = "logit"),
data = credit1)
# modèle d'apres anova et Wald test
model_Anova_perso <-glm(class ~status_account+duration+history+purpose+savings
+installement_rate+status_personnal+other_debtors+ other_installment,
data = credit1, family = binomial(link="cloglog"))
#modèle d'apres  Step-min_AIc logit
model_Step <- glm(formula = class ~ status_account + duration + history + purpose +
amount + savings + employment + installement_rate + status_personnal +
other_debtors + age + other_installment + housing, family = binomial(link = "logit"),
data = credit1)
# modèle d'apres anova et Wald test
model_Anova_perso <-glm(class ~status_account+duration+history+purpose+savings
+installement_rate+status_personnal+other_debtors+ other_installment,
data = credit1, family = binomial(link="cloglog"))
summary(model_Step)
summary(model_Anova_perso)
#Donc on va choisir le modèle Anova-perso
#qui a une link fonction de type cloglog
model_Anova_perso
par(mfrow=c(1,1))
set.seed(132)
train.samples1=createDataPartition(credit1$class,p =.8, list =F)#train 80%
train1<- credit1[train.samples1, ] # 80%
test1 <- credit1[-train.samples1, ] # 20%
model_Anova_perso <-glm(class ~status_account+duration+history+purpose+savings
+installement_rate+status_personnal+other_debtors+ other_installment,
data = train1, family = binomial(link="cloglog"))
probabilities <- predict(model_Anova_perso,test1, type = "response")#prediction le test 20%
probabilities
predicted.class <- ifelse(probabilities > 0.5, 1, 0)# Good ou Bad credit
Acc_gen =mean(predicted.class==test1$class) #Accuracy
Acc_gen
pred.rocr <- prediction(probabilities, test1$class)
eval <- performance(pred.rocr, "acc")
plot(eval)
max <- which.max(slot(eval, "y.values")[[1]])
acc <- slot(eval, "y.values")[[1]][max] #y.values are accuracy
#measures
cut <- slot(eval, "x.values")[[1]][max] #x.values are cutoff
#measures
print(c(Accuracy = acc, Cutoff = cut))
Acc_gen =mean(predicted.class==test1$class) #Accuracy
Acc_gen #p =0.5173151
#et donc pour mieux prdire il faut que p > 0.6361595
predicted.class <- ifelse(probabilities > 0.5173151 , 1, 0)
Acc_gen =mean(predicted.class==test1$class) #Accuracy
Acc_gen #p =0.5173151
model_Anova_perso
# Monto Carlo de logit
data_sel.logit<-Trying_all(credit1,"logit",n.iter=100)#Attention ca durer 8 min
# Monto Carlo de probit
data_sel.probit<-Trying_all(credit1,"probit",n.iter=100)#Attention ca durer 8 min
# Monto Carlo de cloglog
data_sel.cloglog<-Trying_all(credit1,"cloglog",n.iter=100)#Attention ca durer 8 min
